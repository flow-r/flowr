---
title: "flowr simple examples"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{flowr simple example}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE, message = FALSE}
library(knitr)
knitr::opts_chunk$set(
  comment = "#>",
  error = FALSE,
  tidy = FALSE,
	fig.cap = ""
)
library(flowr)

```

An easy quick way to build a workflow is create two seperate files. The first is a table with commands to run the other has details regarding how the modules are stitched together.


## Styles

```{r}
exdata = file.path(system.file(package = "flowr"), "extdata")
flow_mat = read_sheet(file.path(exdata, "example1_flow_mat.txt"))
flow_def = read_sheet(file.path(exdata, "example1_flow_def.txt"))
```

### Style 1

#### Ingredient 1: Flow Definition format
Each row of this table translate to a call to ([job](http://docs.flowr.space/build/html/rd/topics/job.html) or) [queue](http://docs.flowr.space/build/html/rd/topics/queue.html) function.


- jobname: is passed as `name` argument to job().
- prev_jobs: passed as `previous_job` argument  to job().
- dep_type: passed as `dependency_type` argument  to job(). Possible values: gather, serial
- sub_type: passed as `submission_type` argument  to job().
- queue: name of the queue to be used for this particular job. 
	Since each jobs can be submitted to a different queue, this makes your flow very flexible
- memory_reserved: Refer to your system admin guide on what values should go here. 
	Some examples: 160000, 16g etc representing a 16GB reservation of RAM
- walltime: How long would this job run. Again refer to your HPCC guide. Example: 24:00, 24:00:00
- cpu_reserved: Amount of CPU reserved.

Its best to have this as a tab seperated file (with no row.names).

```{r}
kable(head(flow_def))
```

#### Ingredient 2: flow_mat: A table with all the commands to run:

- samplename: The idea is to group commands based on this column and submit individual flows for each sample
	- This column is not really used in subsequent steps. So one can put a dummy value like sample1, for all the rows if no subsetting is desired
- jobname: This corresponds to the name of the job. This should match perfectly with the jobname column in flow_def
- cmd: the command to run

Its best to have this as a tab seperated file (with no row.names)

```{r}
kable(subset(flow_mat, samplename == "sample1"))
```



### Style 2

This style may be more suited for people who like to explore more advanced usage and like to code in R. Also this one find this much faster if jobs and their relationships changes a lot.

Here instead of seperating cmds and definitions one defines them step by step incrementally.

- Use: queue(), to define the computing cluster being used
- Use: multiple calls job()
- Use: flow() to stich the jobs into a flow.


Currently we support LSF, Torque and SGE. Let us use LSF for this example.

```{r getqobj}
qobj <- queue(platform = "lsf", queue = "normal", verbose = FALSE)
```

Let us stitch a simple flow with three jobs, which are submitted one after the other.

```{r plot_simpleflow}
job1 <- job(name = "myjob1", cmds = "sleep1", q_obj = qobj)
job2 <- job(name = "myjob2", cmds = "sleep2", q_obj = qobj, previous_job = "myjob1", dependency_type = "serial")
job3 <- job(name = "myjob3", cmds = "sleep3", q_obj = qobj, previous_job = "myjob1", dependency_type = "serial")
fobj <- flow(name = "myflow", jobs = list(job1, job2, job3), desc="description")
plot_flow(fobj)
```

The above translates to a flow definition which looks like this:

```{r}
dat <- flowr:::create_jobs_mat(fobj)
knitr:::kable(dat)
```



## Submission types
- scatter: submit all commands as parallel independent jobs
- serial: run these commands sequentuially one after the other

## Dependency types
- none: independent job
- serial: *one to one* relationship with previous job
- gather: *many to one* wait for **all** commands in previous job to finish then start current
- burst: *one to many* wait for one jobs and start several when it completes

## Relationships

### Serial: one to one relationship

- All commads in 'job1' are submitted, and those is 'jobs2' *wait* for those in 'job1' to complete.
- Commands in 'job2' are serially dependent on 'job1'
- Both jobs are submitted as parallel (*scatter*), i.e. there is not **intra** dependency.
- so previous job submission: `scatter`, and current job's dependency type `serial`
```{r}
cmds = rep("sleep 5", 10)
jobj1 <- job(q_obj=qobj, cmd = cmds, submission_type = "scatter", name = "job1")
jobj2 <- job(q_obj=qobj, name = "job2", cmd = cmds, submission_type = "scatter", 
             dependency_type = "serial", previous_job = "job1")
fobj <- flow(jobs = list(jobj1, jobj2))
plot_flow(fobj)
```

### Gather: many to one relationship

- makes sense when previous job had many commands running in parallel and current job would wait for all
- so previous job submission: `scatter`, and current job's dependency type `gather`
```{r}
jobj1 <- job(q_obj=qobj, cmd = cmds, submission_type = "scatter", name = "job1")
jobj2 <- job(q_obj=qobj, name = "job2", cmd = cmds, submission_type = "scatter", 
             dependency_type = "gather", previous_job = "job1")
fobj <- flow(jobs = list(jobj1, jobj2))
plot_flow(fobj)
```

### Burst: one to many relationship
- makes sense when previous job had one command current job would split and submit several jobs in parallel
- so previous job submission_type: `serial`, and current job's dependency type `burst`, with a submission type: `scatter`
```{r}
jobj1 <- job(q_obj=qobj, cmd = cmds, submission_type = "serial", name = "job1")
jobj2 <- job(q_obj=qobj, name = "job2", cmd = cmds, submission_type = "scatter", 
             dependency_type = "burst", previous_job = "job1")
fobj <- flow(jobs = list(jobj1, jobj2))
plot_flow(fobj)
```


## HPCC submission formats

**LSF**
```{r}
queue(type = "lsf")@format
```

**torque**
```{r}
queue(type = "torque")@format
```

My HPCC is not supported, how to make it work? send a message to: sahil.seth [at] me.com


